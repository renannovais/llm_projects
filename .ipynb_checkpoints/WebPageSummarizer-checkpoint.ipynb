{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f8c109d-a4a7-4eac-98e0-073e77a591da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6f2b00a-a07c-4d8f-83c7-2b57a38a847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env variables\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OPENAI_API_KEY not found in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aade1349-147c-4860-bf92-1b05f45e74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating object to work with GPT tasks \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e8a2ec6d-fbba-45bd-b59c-0899392b5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It looks like your API is working just fine. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "# Checking API response\n",
    "message = \"Hello, GPT! This is a test message to make sure my API works.\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "394aa500-e1e7-4a7c-9e10-d795380b6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to work with text extraction, processing and summarizing from a given url\n",
    "class WebPageSummarizer():\n",
    "    def __init__(self, url: str, show_summary: bool = True):\n",
    "        \"\"\"\n",
    "        Class to work with text extraction, processing and summarizing from a given url using the BeautifulSoup library.\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        self.url = None\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        else:\n",
    "            self.url = url\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Try to extract meaningful content\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        # Join all paragraph text\n",
    "        self.text = \"\\n\".join([p.get_text() for p in paragraphs if p.get_text().strip() != \"\"])\n",
    "\n",
    "        if show_summary:\n",
    "            print(\"üîó Preview of extracted text:\\n\")\n",
    "            print(self.text[:500] + \"\\n...\\n\")\n",
    "            print(f\"Amount of words: {len(self.text.split())}\")\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"Method to process user prompts in the context of the user.\"\"\"\n",
    "        \n",
    "        # Prompt for system definition\n",
    "        self.system_prompt = \"\"\" You are an assistant that analyzes the contents of a website and provides a summary. \n",
    "        The detail of the summary may depend on the content you may find. Expect to find both complex but also simple contents.\n",
    "        If you find text that might be navigation related or ad related please ignore. Respond in markdown. Also, can you please start your summary \n",
    "        with the tile \"üìù Summary\"?\n",
    "        \n",
    "        If possible, please show cheerful behavior during your summary, something like during summary: \"Furthermore, it builds Symmetric Trees (or Oblivious Trees) \n",
    "        to handle splits uniformly at each tree level, enhancing performance and interpretability.\" and then you commne something like: \"Isn't that interesting?\". \n",
    "        Thanks for your help!\"\"\"\n",
    "\n",
    "        self.content = f\"\"\"The text to summarize is as follows: {self.text}\"\"\"\n",
    "\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"system\", \"content\":self.system_prompt}, \n",
    "                                                                                 {\"role\": \"user\",  \"content\":self.content}])\n",
    "        print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6439dbf-0b96-4638-8c1b-dd3001573ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Preview of extracted text:\n",
      "\n",
      "Deep & Shallow\n",
      "While XGBoost and LightGBM reigned the ensembles in Kaggle competitions, another contender took its birth in Yandex, the Google from Russia. It decided to take the path less tread, and took a different approach to Gradient Boosting. They sought to fix a key problem, as they see it, in all the other GBMs in the world.\n",
      "Let‚Äôs take a look at what made it different:\n",
      "Let‚Äôs take a look at the innovation which gave the algorithm it‚Äôs name ‚Äì CatBoost. Unlike XGBoost, CatBoost deals with Ca\n",
      "...\n",
      "\n",
      "Amount of words: 1615\n"
     ]
    }
   ],
   "source": [
    "web_page_summarizer = WebPageSummarizer(\"https://deep-and-shallow.com/2020/02/29/the-gradient-boosters-v-catboost/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdd3b5ba-e51f-422d-aff8-4df22929aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# üìù Summary\n",
      "\n",
      "CatBoost, developed by Yandex, takes a fresh and innovative approach to Gradient Boosting, particularly by handling categorical variables natively, which differentiates it from competitors like XGBoost and LightGBM. Isn't that fascinating? Unlike traditional methods that often struggle with high cardinality categorical features, CatBoost embraces the concept of Ordered Target Statistics. This helps mitigate issues of target leakage, ensuring that the model doesn't overfit to the training data by treating target statistics sequentially in a creatively constructed \"artificial time\".\n",
      "\n",
      "The authors highlight the problems of traditional gradient boosting, such as the tendency to reuse the same dataset for each iteration, leading to target leakage. CatBoost introduces a practical solution through Ordered Boosting ‚Äî generating multiple permutations of the dataset to create a robust model that reduces variance in predictions. \n",
      "\n",
      "Moreover, CatBoost builds Symmetric Trees (or Oblivious Trees), where the same features are used for splitting at each tree level, enhancing performance and interpretability. Additionally, it implicitly considers combinations of categorical variables, which allows for more nuanced feature representation without overwhelming complexity. \n",
      "\n",
      "A significant feature is the inbuilt Overfitting Detector, which can automatically halt training before the maximum number of iterations is reached, thus optimizing performance. CatBoost also provides flexible handling of missing values and offers a rich set of hyperparameters, making it adaptable yet complex. \n",
      "\n",
      "Overall, CatBoost represents a significant advancement in the gradient boosting landscape, addressing issues that its predecessors haven't fully overcome. It's indeed a very enlightening read!\n"
     ]
    }
   ],
   "source": [
    "web_page_summarizer.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "93363d36-8b5d-4ed8-b612-7df3bf7320e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Detected language: PT\n",
      "üîó Preview of extracted text:\n",
      "\n",
      "ITASAT2 ir√° atuar para aplica√ß√µes cient√≠ficas e de defesa\n",
      "Publicado em 14/04/2025 - 14h15\n",
      "O Instituto Tecnol√≥gico de Aeron√°utica (ITA) realizou, entre os dias 17 e 19 de mar√ßo, a Revis√£o Preliminar de Projeto (PDR) do ITASAT 2, novo microssat√©lite em desenvolvimento por pesquisadores do Centro Espacial ITA (CEI). A atividade representa uma etapa importante dos estudos e contou com a presen√ßa de institui√ß√µes parceiras, tanto do Brasil quanto do exterior.\n",
      "Participaram do encontro representantes do\n",
      "...\n",
      "\n",
      "Amount of words: 526\n",
      "# üìù Summary\n",
      "\n",
      "Exciting news from the Instituto Tecnol√≥gico de Aeron√°utica (ITA)! They recently conducted a Preliminary Design Review (PDR) for their new microsatellite, ITASAT 2, which is set to focus on scientific and defense applications! üéâ The PDR took place from March 17 to 19 and involved a collaboration of international partners, including NASA and the U.S. Naval Research Laboratory, showcasing the project's broad support and knowledge-sharing.\n",
      "\n",
      "Scheduled for launch in the coming years, ITASAT 2 will consist of a constellation of three CubeSats dedicated to monitoring the Earth's ionosphere‚Äîwho knew satellites could be used for such intricate observations? üåå These satellites will help evaluate the formation and impact of plasma bubbles and even aid in locating radiofrequency sources on land and at sea for defense purposes!\n",
      "\n",
      "The review highlighted the team's competence, with unanimous approval to continue to the next phase‚Äîwhat a fantastic achievement! ü§© It required expertise from multiple engineering fields, demonstrating the depth of skill behind such projects. Additionally, the ITA has a rich history of satellite development, previously launching ITASAT 1 in 2018 and the SPORT satellite in 2022. \n",
      "\n",
      "There's even more on the horizon with the CEI's development of SelenITA, another CubeSat that will contribute to NASA's Artemis Mission for lunar studies. Absolutely thrilling times for space exploration and education! üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, LangDetectException\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OPENAI_API_KEY not found in .env file.\")\n",
    "\n",
    "# Generating object to work with GPT tasks \n",
    "openai = OpenAI()\n",
    "\n",
    "# Class to work with text extraction, processing and summarizing from a given url\n",
    "class WebPageSummarizer():\n",
    "    def __init__(self, url: str, summary_detail: str = \"high\", show_summary: bool = True, language_of_reference = \"English\") -> None:\n",
    "        \"\"\"\n",
    "        Class to work with text extraction, processing and summarizing from a given url using the BeautifulSoup library.\n",
    "        \"\"\"\n",
    "\n",
    "        LANGUAGE_CODE_MAP = {\n",
    "            \"english\": \"en\",\n",
    "            \"portuguese\": \"pt\",\n",
    "            \"spanish\": \"es\",\n",
    "            \"french\": \"fr\",\n",
    "            \"german\": \"de\",\n",
    "            \"italian\": \"it\",\n",
    "            \"japanese\": \"ja\",\n",
    "            \"chinese\": \"zh\",\n",
    "            \"korean\": \"ko\",\n",
    "        }\n",
    "\n",
    "        self.summary_detail = summary_detail\n",
    "        self.url = None\n",
    "        self.language_of_reference = language_of_reference\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        else:\n",
    "            self.url = url\n",
    "\n",
    "        if self.summary_detail not in [\"high\", \"low\"]:\n",
    "            raise Exception(\"\"\"Please select summary detail as either \"high\" or \"low\". \"\"\")\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Try to extract meaningful content\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        # Join all paragraph text\n",
    "        self.text = \"\\n\".join([p.get_text() for p in paragraphs if p.get_text().strip() != \"\"])\n",
    "\n",
    "        # Guarantee limit of text to summary\n",
    "        max_words = 7000\n",
    "        if len(self.text.split()) > max_words:\n",
    "            self.text = \" \".join(self.text.split()[:max_words])\n",
    "\n",
    "        try:\n",
    "            self.language_url = detect(self.text)\n",
    "        except LangDetectException:\n",
    "            self.language_url = \"unknown\"\n",
    "\n",
    "        # Normalize and resolve target language code\n",
    "        target_language_name = self.language_of_reference.lower().strip()\n",
    "        self.target_language_code = LANGUAGE_CODE_MAP.get(target_language_name)\n",
    "        \n",
    "        if not self.target_language_code:\n",
    "            raise ValueError(f\"‚ùå Unsupported language: {self.language_of_reference}. Please use one of: {list(LANGUAGE_CODE_MAP.keys())}\")\n",
    "\n",
    "        print(f\"üåç Detected language: {self.language_url.upper()}\")\n",
    "        \n",
    "        if show_summary:\n",
    "            print(\"üîó Preview of extracted text:\\n\")\n",
    "            print(self.text[:500] + \"\\n...\\n\")\n",
    "            print(f\"Amount of words: {len(self.text.split())}\")\n",
    "\n",
    "    def summarize(self)-> str:\n",
    "        \"\"\"Method to process user prompts in the context of the user.\"\"\"\n",
    "        \n",
    "        # Prompt for system definition\n",
    "        self.system_prompt = f\"\"\" \n",
    "        You are an assistant that analyzes the contents of a website and provides a summary. \n",
    "        Please notice that providing a {self.summary_detail} summary detail is IMPORTANT.\n",
    "        If you find text that might be navigation related or ad related please ignore. Respond in markdown. Also, can you please start your summary \n",
    "        with the tile \"üìù Summary\"?\n",
    "        \n",
    "        Please show some excited behavior during your summary, making comments with extra knowledge if possible during or at the end of the sentence. \n",
    "        \"\"\"\n",
    "\n",
    "        self.content = f\"\"\"The text to summarize is as follows: {self.text}\"\"\"\n",
    "\n",
    "        if self.language_url != self.target_language_code:\n",
    "            self.system_prompt = f\"\"\"The website content is in {self.language_url.upper()}. Please first translate it to {self.language_of_reference}. \n",
    "            {self.system_prompt.strip()}\n",
    "            \"\"\"\n",
    "\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"system\", \"content\":self.system_prompt}, \n",
    "                                                                                 {\"role\": \"user\",  \"content\":self.content}])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "web_page_summarizer = WebPageSummarizer(\"http://www.ita.br/noticias/revisodeprojetodonovomicrossatlitedoitaaprovada\", summary_detail = \"low\")\n",
    "print(web_page_summarizer.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8ce13728-0040-43cc-82cd-e10c838ef71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Detected language: PT\n",
      "üîó Preview of extracted text:\n",
      "\n",
      "ITASAT2 ir√° atuar para aplica√ß√µes cient√≠ficas e de defesa\n",
      "Publicado em 14/04/2025 - 14h15\n",
      "O Instituto Tecnol√≥gico de Aeron√°utica (ITA) realizou, entre os dias 17 e 19 de mar√ßo, a Revis√£o Preliminar de Projeto (PDR) do ITASAT 2, novo microssat√©lite em desenvolvimento por pesquisadores do Centro Espacial ITA (CEI). A atividade representa uma etapa importante dos estudos e contou com a presen√ßa de institui√ß√µes parceiras, tanto do Brasil quanto do exterior.\n",
      "Participaram do encontro representantes do\n",
      "...\n",
      "\n",
      "Amount of words: 526\n",
      "\n",
      "\n",
      "# üìù Summary\n",
      "\n",
      "The ITASAT2 project, undertaken by the Aeronautical Institute of Technology (ITA), focuses on scientific and defense applications! Between March 17-19, 2025, a Preliminary Design Review (PDR) was conducted, a crucial step involving both Brazilian and international partners like NASA and the U.S. Southern Command. Excitingly, ITASAT 2 will be a constellation of three CubeSats aimed at monitoring the Earth‚Äôs ionosphere and will also be capable of geolocating radio frequency sources, enhancing defense capabilities.\n",
      "\n",
      "The PDR marked an important technical and managerial milestone, showcasing the team's understanding of project objectives and risk identification. With unanimous approval from the review committee, the project is set to progress, demonstrating the diverse engineering expertise involved, from orbital mechanics to thermal management!\n",
      "\n",
      "Additionally, the CubeSat structure is particularly fascinating! The ITASAT 2 will utilize a 16U design, integrating 16 cubic units, highlighting ITA's rich history in satellite development‚Äîincluding the operational ITASAT 1 and the recently launched SPORT. Furthermore, ITA is also developing the SelenITA CubeSat for NASA's Artemis mission to study the Moon. How cool is that? üåï‚ú®\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, LangDetectException\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OPENAI_API_KEY not found in .env file.\")\n",
    "\n",
    "# Generating object to work with GPT tasks \n",
    "openai = OpenAI()\n",
    "\n",
    "# Class to work with text extraction, processing and summarizing from a given url\n",
    "class WebPageSummarizer():\n",
    "    \"\"\"\n",
    "        Class to work with text extraction, processing and summarizing from a given url using the BeautifulSoup library.\n",
    "    \"\"\"\n",
    "    def __init__(self, url: str, summary_detail: str = \"high\", show_summary: bool = True, language_of_reference = \"English\") -> None:\n",
    "\n",
    "        # Initial summarizer settings\n",
    "        self.url = url\n",
    "        self.show_summary = show_summary\n",
    "        self.summary_detail = summary_detail\n",
    "        self.language_of_reference = language_of_reference\n",
    "        self.language_code_map = {\n",
    "            \"english\": \"en\",\n",
    "            \"portuguese\": \"pt\",\n",
    "            \"spanish\": \"es\",\n",
    "            \"french\": \"fr\",\n",
    "            \"german\": \"de\",\n",
    "            \"italian\": \"it\",\n",
    "            \"japanese\": \"ja\",\n",
    "            \"chinese\": \"zh\",\n",
    "            \"korean\": \"ko\",\n",
    "        }\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        if self.summary_detail not in [\"high\", \"low\"]:\n",
    "            raise Exception(\"\"\"Please select summary detail as either \"high\" or \"low\".\"\"\")\n",
    "\n",
    "    def __extract_text(self):\n",
    "        response = requests.get(self.url, headers=self.headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Try to extract meaningful content\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        # Join all paragraph text\n",
    "        self.text = \"\\n\".join([p.get_text() for p in paragraphs if p.get_text().strip() != \"\"])\n",
    "\n",
    "        # Guarantee limit of text to summary\n",
    "        max_words = 7000\n",
    "        if len(self.text.split()) > max_words:\n",
    "            self.text = \" \".join(self.text.split()[:max_words])\n",
    "    \n",
    "    def __detect_language(self):\n",
    "        # Detect language\n",
    "        try:\n",
    "            self.language_url = detect(self.text)\n",
    "        except LangDetectException:\n",
    "            self.language_url = \"unknown\"\n",
    "\n",
    "        # Normalize and resolve target language code\n",
    "        target_language_name = self.language_of_reference.lower().strip()\n",
    "        self.target_language_code = self.language_code_map.get(target_language_name)\n",
    "        \n",
    "        if not self.target_language_code:\n",
    "            raise ValueError(f\"‚ùå Unsupported language: {self.language_of_reference}. Please use one of: {list(LANGUAGE_CODE_MAP.keys())}\")\n",
    "\n",
    "        print(f\"üåç Detected language: {self.language_url.upper()}\")\n",
    "        \n",
    "        if self.show_summary:\n",
    "            print(\"üîó Preview of extracted text:\\n\")\n",
    "            print(self.text[:500] + \"\\n...\\n\")\n",
    "            print(f\"Amount of words: {len(self.text.split())}\\n\\n\")\n",
    "\n",
    "    def summarize(self)-> str:\n",
    "        \"\"\"Method to process user prompts in the context of the user.\"\"\"\n",
    "        self.__extract_text()\n",
    "        self.__detect_language()\n",
    "        \n",
    "        # Prompt for system definition\n",
    "        self.system_prompt = f\"\"\" \n",
    "        You are an assistant that analyzes the contents of a website and provides a summary. \n",
    "        Please notice that providing a {self.summary_detail} summary detail is IMPORTANT.\n",
    "        If you find text that might be navigation related or ad related please ignore. Respond in markdown. Also, can you please start your summary \n",
    "        with the tile \"üìù Summary\"?\n",
    "        \n",
    "        Please show some excited behavior during your summary, making comments with extra knowledge if possible during or at the end of the sentence. \n",
    "        \"\"\"\n",
    "\n",
    "        self.content = f\"\"\"The text to summarize is as follows: {self.text}\"\"\"\n",
    "\n",
    "        if self.language_url != self.target_language_code:\n",
    "            self.system_prompt = f\"\"\"The website content is in {self.language_url.upper()}. Please first translate it to {self.language_of_reference}. \n",
    "            {self.system_prompt.strip()}\n",
    "            \"\"\"\n",
    "\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"system\", \"content\":self.system_prompt}, \n",
    "                                                                                 {\"role\": \"user\",  \"content\":self.content}])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "web_page_summarizer = WebPageSummarizer(\"http://www.ita.br/noticias/revisodeprojetodonovomicrossatlitedoitaaprovada\", summary_detail = \"low\")\n",
    "print(web_page_summarizer.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa722e8-fb4f-4633-af45-a195e46d7545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
